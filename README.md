# Real-Time Talking Avatar System ğŸ­

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

**Open-Source Real-Time High-Quality Avatar System with Machine Learning/Deep Learning**

This project generates high-quality talking avatar videos from speech audio using fully open-source tools and deep learning models. Built as part of ATG Task 3 technical assignment.

## ğŸ¯ Features

- ğŸ¤ **Audio-to-Avatar Pipeline**: Convert speech to animated avatar
- ğŸš€ **Real-Time Capable**: 30 FPS generation (GPU-accelerated)
- ğŸ‘¤ **Realistic Rendering**: Face, eyes, nose, mouth, eyebrows with natural motion
- ğŸ¬ **Video Generation**: Multiple format support (MP4, AVI, GIF)
- ğŸ”§ **Production Ready**: Docker + Kubernetes deployment configs
- ğŸ“Š **Comprehensive Metrics**: Performance benchmarking and quality evaluation
- ğŸŒ **REST API**: FastAPI-based inference server

## ğŸ—ï¸ Architecture

```
Audio Input (16kHz WAV)
    â†“
Speech Encoder (Conv1D) â†’ 256-dim features
    â†“
Expression Model (MLP) â†’ 64-dim expression params
    â†“
Motion Model (Separate nets) â†’ Head (3D) + Eye (2D) motion
    â†“
Neural Renderer (PIL) â†’ RGB Frame (256Ã—256)
```

## ğŸ¬ Demo Output

### Generated Avatar
![Avatar Demo](demo/avatar_demo.png)

### Video Generation
![Avatar Animation](demo/avatar_demo.gif)

*3-second talking avatar generated from speech-like audio at 30 FPS*

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yuvakavi/ATG-Task-3.git
cd ATG-Task-3

# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\activate

# Activate (Linux/Mac)
source .venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### 1. Generate Test Audio
```bash
python create_demo_audio.py --output demo/demo_audio.wav --duration 3
```

### 2. Generate Single Frame
```bash
python main.py --audio demo/demo_audio.wav --output avatar.png
```

### 3. Generate Video
```bash
python demo/app.py --audio demo/demo_audio.wav --output demo/output.mp4 --fps 30
```

### 4. View Results
```bash
# Open interactive viewer in browser
cd demo
start viewer.html
```

## ğŸ“– Advanced Usage

### Run Complete Pipeline
```bash
python main.py --audio your_audio.wav --output result.png
```

### Run API Server
```bash
python api/server.py
```
Then visit http://localhost:8000/docs for API documentation

### Generate Video with Audio Track (requires FFmpeg)
```bash
python generate_video_with_audio.py --audio demo_audio.wav --output final.mp4
```

### Use LJ Speech Dataset
```bash
python demo_with_dataset.py --index 0 --output avatar_sample.png
```

### Convert Video Formats
```bash
# To AVI (high compatibility)
python demo/convert_video.py --input video.mp4 --output video.avi --codec XVID

# To GIF (universal)
python demo/video_to_gif.py --input video.mp4 --output animation.gif
```

## ğŸ“ Project Structure

```
avatar-system/
â”œâ”€â”€ models/              # Neural network models
â”‚   â”œâ”€â”€ speech_encoder/  # Audio feature extraction
â”‚   â”œâ”€â”€ expression_model/# Expression parameter generation
â”‚   â”œâ”€â”€ motion_model/    # Head and eye motion
â”‚   â””â”€â”€ renderer/        # Avatar frame rendering
â”œâ”€â”€ inference/           # Real-time pipeline
â”œâ”€â”€ preprocessing/       # Audio processing
â”œâ”€â”€ api/                # FastAPI REST server
â”œâ”€â”€ demo/               # Demo applications
â”œâ”€â”€ evaluation/         # Metrics and benchmarking
â”œâ”€â”€ deployment/         # Docker & Kubernetes configs
â””â”€â”€ configs/            # YAML configurations
```

## ğŸ¨ Avatar Features

The generated avatar includes:
- ğŸ‘¤ **Face**: Skin-toned oval with head motion
- ğŸ‘€ **Eyes**: Animated pupils with gaze tracking
- ğŸ‘ƒ **Nose**: Centered facial feature
- ğŸ‘„ **Mouth**: Opens/closes based on speech
- âœï¸ **Eyebrows**: Move with expression changes

## ğŸ“Š Technical Specifications

| Component | Details |
|-----------|---------|
| **Input** | 16kHz mono WAV audio |
| **Output** | 256Ã—256 RGB frames @ 30 FPS |
| **Models** | PyTorch-based neural networks |
| **API** | FastAPI with async support |
| **Deployment** | Docker + Kubernetes ready |

## ğŸ“š Documentation

- [Installation Guide](INSTALL.md) - Detailed setup instructions
- [Quick Start Guide](QUICKSTART.md) - Get started in 5 minutes
- [Video Generation Guide](demo/VIDEO_README.md) - Create videos
- [Contributing Guidelines](CONTRIBUTING.md) - How to contribute
- [Changelog](CHANGELOG.md) - Version history

## ğŸ§ª Testing

```bash
python test_workspace.py
```

## ğŸ“ˆ Performance

- **GPU (T4)**: ~30-60 FPS (real-time capable)
- **CPU (8 cores)**: ~5-10 FPS
- **Memory**: ~2GB GPU VRAM, ~4GB system RAM

## ğŸš€ Deployment

### Docker
```bash
cd deployment/docker
docker build -t avatar-system .
docker run -p 8000:8000 avatar-system
```

### Kubernetes
```bash
kubectl apply -f deployment/kubernetes/
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- PyTorch team for deep learning framework
- OpenCV for image processing
- librosa for audio processing
- FastAPI for web framework

## ğŸ‘¨â€ğŸ’» Author

**Yuva Kavi**
- GitHub: [@yuvakavi](https://github.com/yuvakavi)
- Repository: [ATG-Task-3](https://github.com/yuvakavi/ATG-Task-3)

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Task**: ATG Task 3 - Technical Assignment  
**Date**: January 2026  
**Status**: âœ… Complete
